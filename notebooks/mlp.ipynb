{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Import MINST data\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# set parameters\n",
    "learning_rate = 0.001\n",
    "training_epochs = 15\n",
    "batch_size = 100\n",
    "display_step = 1\n",
    "\n",
    "# network parameters\n",
    "n_hidden_1 = 256 # 1st layer number of features\n",
    "n_hidden_2 = 256 # 2nd layer number of features\n",
    "n_input = 784 # MNIST data input (img shape: 28*28)\n",
    "n_classes = 10 # MNIST total classes (0-9 digits)\n",
    "\n",
    "# tf Graph input\n",
    "x = tf.placeholder(\"float\", [None, n_input])\n",
    "y = tf.placeholder(\"float\", [None, n_classes])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def multilayer_perceptron(x, weights, biases):\n",
    "    # hidden layer with RELU activation\n",
    "    layer_1 = tf.add(tf.matmul(x, weights['h1']), biases['b1'])\n",
    "    layer_1 = tf.nn.relu(layer_1)\n",
    "    \n",
    "    # hidden layer with RELU activation\n",
    "    layer_2 = tf.add(tf.matmul(layer_1, weights['h2']), biases['b2'])\n",
    "    layer_2 = tf.nn.relu(layer_2)\n",
    "    \n",
    "    # output layer with linear activation\n",
    "    out_layer = tf.matmul(layer_2, weights['out']) + biases['out']\n",
    "    return out_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# store layers weight & bias\n",
    "weights = {\n",
    "    'h1': tf.Variable(tf.random_normal([n_input, n_hidden_1])),\n",
    "    'h2': tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2])),\n",
    "    'out': tf.Variable(tf.random_normal([n_hidden_2, n_classes]))\n",
    "}\n",
    "biases = {\n",
    "    'b1': tf.Variable(tf.random_normal([n_hidden_1])),\n",
    "    'b2': tf.Variable(tf.random_normal([n_hidden_2])),\n",
    "    'out': tf.Variable(tf.random_normal([n_classes]))\n",
    "}\n",
    "\n",
    "# construct model\n",
    "pred = multilayer_perceptron(x, weights, biases)\n",
    "\n",
    "# define loss and optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(pred, y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "# initializing the variables\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Session config\n",
    "\n",
    "Basically tensorflow take all the available GPU memory, it is not good way when you run tensorflow in server. So in that case you should give argument when create session called `tf.Config`.<br>\n",
    "In `tf.Config` there is some options like `allow_growth` and it means that tensorflow take VRAM when program need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 cost=162.32304\n",
      "Epoch: 0002 cost=43.73151\n",
      "Epoch: 0003 cost=27.30590\n",
      "Epoch: 0004 cost=18.99481\n",
      "Epoch: 0005 cost=14.06471\n",
      "Epoch: 0006 cost=10.33647\n",
      "Epoch: 0007 cost=7.81675\n",
      "Epoch: 0008 cost=5.79247\n",
      "Epoch: 0009 cost=4.32427\n",
      "Epoch: 0010 cost=3.30768\n",
      "Epoch: 0011 cost=2.61041\n",
      "Epoch: 0012 cost=1.83468\n",
      "Epoch: 0013 cost=1.35407\n",
      "Epoch: 0014 cost=1.32741\n",
      "Epoch: 0015 cost=0.97335\n",
      "Optimization Finished!\n",
      "Train accuracy: 0.991163969039917\n",
      "Test accuracy: 0.9474000930786133\n"
     ]
    }
   ],
   "source": [
    "# session config\n",
    "config = tf.ConfigProto(gpu_options=tf.GPUOptions(allow_growth=True))\n",
    "\n",
    "sess = tf.Session(config=config)\n",
    "sess.run(init)\n",
    "\n",
    "for epoch in range(training_epochs):\n",
    "    avg_cost = 0.\n",
    "    total_batch = int(mnist.train.num_examples/batch_size)\n",
    "\n",
    "    for i in range(total_batch):\n",
    "        batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "        _, c = sess.run([optimizer, cost], feed_dict={x: batch_x,\n",
    "                                                      y: batch_y})\n",
    "\n",
    "        avg_cost += c / total_batch\n",
    "    if epoch % display_step == 0:\n",
    "        print(\"Epoch: {0:04d} cost={1:.5f}\" .format(epoch+1, avg_cost))\n",
    "print(\"Optimization Finished!\")\n",
    "\n",
    "# test model\n",
    "correct_prediction = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n",
    "# calculate accuracy\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "print(\"Train accuracy: {0}\" \n",
    "      .format(sess.run(accuracy, feed_dict={x: mnist.train.images, y: mnist.train.labels})))\n",
    "print(\"Test accuracy: {0}\" \n",
    "      .format(sess.run(accuracy, feed_dict={x: mnist.test.images, y: mnist.test.labels})))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Result\n",
    "\n",
    "Accuracy of Logistic regression was 0.889<br>\n",
    "Accuracy of MLP is **0.948**\n",
    "\n",
    "But maybe, with some hyper-parameter tuning(learning rate?) and more deeper layer, you can achieve better result.<br>\n",
    "Try it yourself!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# code in here...."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overfit problem?\n",
    "\n",
    "But if stack more and more layers, it occur overfit problem. That's why it it's hart to train neural net.<br>\n",
    "However with some algorithm we can avoid overfit, like **dropout** or **good initialization**.\n",
    "In this code I will show how to use dropout or another initialization in tensorflow. You can use this method in your *deep MNIST* or not.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# tensorflow contrib contain contributed code (maybe most of them merged someday into tensorflow internal)\n",
    "from tensorflow.contrib.layers import variance_scaling_initializer, xavier_initializer\n",
    "\n",
    "# this is Xavier and He initializer\n",
    "xavier_init = xavier_initializer()\n",
    "he_init = variance_scaling_initializer()\n",
    "\n",
    "x = tf.placeholder(\"float\", [None, n_input])\n",
    "y = tf.placeholder(\"float\", [None, n_classes])\n",
    "# note that keep prob in dropout is tf placeholder type\n",
    "# it is because in train it should 0.5 but in test time it must 1.0\n",
    "keep_prob = tf.placeholder(\"float\", None)\n",
    "\n",
    "# get_variable function create Variable if there is no Variable that has some name (e.g. W1, W2...)\n",
    "# otherwise it raise ERROR or if you set reuse=True it return existed Value\n",
    "# so in ipython, it is not good way because all the previous works is stored, \n",
    "# if you have some error I recommend restart ipython kernel\n",
    "weights = {\n",
    "    'h1': tf.get_variable(\"W1\", [n_input, n_hidden_1], initializer=he_init),\n",
    "    'h2': tf.get_variable(\"W2\", [n_hidden_1, n_hidden_2], initializer=he_init),\n",
    "    'out': tf.get_variable(\"W3\", [n_hidden_2, n_classes], initializer=he_init)\n",
    "}\n",
    "biases = {\n",
    "    'b1': tf.Variable(tf.random_normal([n_hidden_1])),\n",
    "    'b2': tf.Variable(tf.random_normal([n_hidden_2])),\n",
    "    'out': tf.Variable(tf.random_normal([n_classes]))\n",
    "}\n",
    "\n",
    "def multilayer_perceptron(x, weights, biases):\n",
    "    # hidden layer with RELU activation and dropout\n",
    "    layer_1 = tf.add(tf.matmul(x, weights['h1']), biases['b1'])\n",
    "    layer_1 = tf.nn.relu(layer_1)\n",
    "    layer_1 = tf.nn.dropout(layer_1, keep_prob)\n",
    "    \n",
    "    # hidden layer with RELU activation and dropout\n",
    "    layer_2 = tf.add(tf.matmul(layer_1, weights['h2']), biases['b2'])\n",
    "    layer_2 = tf.nn.relu(layer_2)\n",
    "    layer_2 = tf.nn.dropout(layer_2, keep_prob)\n",
    "    \n",
    "    # output layer with linear activation and dropout\n",
    "    out_layer = tf.matmul(layer_2, weights['out']) + biases['out']\n",
    "    return out_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 cost=0.64609\n",
      "Epoch: 0002 cost=0.27411\n",
      "Epoch: 0003 cost=0.21550\n",
      "Epoch: 0004 cost=0.18738\n",
      "Epoch: 0005 cost=0.16653\n",
      "Epoch: 0006 cost=0.14790\n",
      "Epoch: 0007 cost=0.13530\n",
      "Epoch: 0008 cost=0.12881\n",
      "Epoch: 0009 cost=0.11582\n",
      "Epoch: 0010 cost=0.11341\n",
      "Epoch: 0011 cost=0.10958\n",
      "Epoch: 0012 cost=0.10085\n",
      "Epoch: 0013 cost=0.10075\n",
      "Epoch: 0014 cost=0.09670\n",
      "Epoch: 0015 cost=0.09417\n",
      "Optimization Finished!\n",
      "Train accuracy: 0.9910548329353333\n",
      "Test accuracy: 0.9784001708030701\n"
     ]
    }
   ],
   "source": [
    "# construct model\n",
    "pred = multilayer_perceptron(x, weights, biases)\n",
    "\n",
    "# define loss and optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(pred, y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "# initializing the variables\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# session config\n",
    "config = tf.ConfigProto(gpu_options=tf.GPUOptions(allow_growth=True))\n",
    "\n",
    "sess = tf.Session(config=config)\n",
    "sess.run(init)\n",
    "\n",
    "for epoch in range(training_epochs):\n",
    "    avg_cost = 0.\n",
    "    total_batch = int(mnist.train.num_examples/batch_size)\n",
    "\n",
    "    for i in range(total_batch):\n",
    "        batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "        _, c = sess.run([optimizer, cost], feed_dict={x: batch_x,\n",
    "                                                      y: batch_y,\n",
    "                                                      keep_prob: 0.5})\n",
    "\n",
    "        avg_cost += c / total_batch\n",
    "    if epoch % display_step == 0:\n",
    "        print(\"Epoch: {0:04d} cost={1:.5f}\" .format(epoch+1, avg_cost))\n",
    "print(\"Optimization Finished!\")\n",
    "\n",
    "# test model\n",
    "correct_prediction = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n",
    "# calculate accuracy\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "print(\"Train accuracy: {0}\" \n",
    "      .format(sess.run(accuracy, feed_dict={x: mnist.train.images, y: mnist.train.labels, keep_prob: 1.0})))\n",
    "print(\"Test accuracy: {0}\" \n",
    "      .format(sess.run(accuracy, feed_dict={x: mnist.test.images, y: mnist.test.labels, keep_prob: 1.0})))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropout works?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# code in here..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
