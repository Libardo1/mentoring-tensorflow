{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load flower dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "daisy 633\n",
      "dandelion 898\n",
      "roses 641\n",
      "sunflowers 699\n",
      "tulips 799\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "DATA_DIR = \"/tmp/flower_photos\"\n",
    "LABELS = [\"daisy\", \"dandelion\", \"roses\", \"sunflowers\", \"tulips\"]\n",
    "\n",
    "for label in LABELS:\n",
    "    l = glob.glob(os.path.join(DATA_DIR, label)+\"/*.jpg\")\n",
    "    print(label, len(l))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although flower dataset has unbalanced data in each label, it might not major problem.<br>\n",
    "So for simplicity, we're goona just shuffle this dataset and split into train/test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'daisy', 'path': '/tmp/flower_photos/daisy/1879567877_8ed2a5faa7_n.jpg'}, {'label': 'daisy', 'path': '/tmp/flower_photos/daisy/3445110406_0c1616d2e3_n.jpg'}, {'label': 'daisy', 'path': '/tmp/flower_photos/daisy/517054463_036db655a1_m.jpg'}, {'label': 'daisy', 'path': '/tmp/flower_photos/daisy/3706420943_66f3214862_n.jpg'}, {'label': 'daisy', 'path': '/tmp/flower_photos/daisy/12193032636_b50ae7db35_n.jpg'}]\n"
     ]
    }
   ],
   "source": [
    "flower_list = list()\n",
    "for label in LABELS:\n",
    "    files = glob.glob(os.path.join(DATA_DIR, label)+\"/*.jpg\")\n",
    "    for file in files:\n",
    "        flower_list.append({\"label\": label, \"path\": file})\n",
    "        \n",
    "print(flower_list[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load all imagese\n",
    "Load image is really slow job. So instead load images in train stage, first load all image in here and just use loaded image in train stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import util\n",
    "\n",
    "for i, flower in enumerate(flower_list):\n",
    "    im = util.load_and_preprocess_image(flower[\"path\"], [224, 224])\n",
    "    flower_list[i][\"img\"] = im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2569 1101\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# split train/test : 0.7/0.3\n",
    "data = dict()\n",
    "\n",
    "random.shuffle(flower_list)\n",
    "num_train = int(len(flower_list) * 0.7)\n",
    "data[\"train\"] = flower_list[:num_train]\n",
    "data[\"test\"]  = flower_list[num_train:]\n",
    "\n",
    "print(len(data[\"train\"]), len(data[\"test\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build CNN using tensorflow.contrib.slim\n",
    "What is slim? Just see [here](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/slim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "slim = tf.contrib.slim\n",
    "\n",
    "def my_arg_scope():\n",
    "    with slim.arg_scope([slim.conv2d, slim.fully_connected],\n",
    "                         activation_fn=tf.nn.relu,\n",
    "                         weights_initializer=slim.variance_scaling_initializer(),\n",
    "                         biases_initializer=tf.zeros_initializer):\n",
    "        with slim.arg_scope([slim.conv2d], padding=\"SAME\") as arg_sc:\n",
    "            return arg_sc\n",
    "\n",
    "        \n",
    "def build_model( inputs, is_training ):\n",
    "    with slim.arg_scope(my_arg_scope()):\n",
    "        net = slim.conv2d(inputs, 64, [3, 3], scope=\"conv1\")\n",
    "        net = slim.max_pool2d(net, [2, 2], scope=\"pool1\")\n",
    "        net = slim.conv2d(net, 128, [3, 3], scope=\"conv2\")\n",
    "        net = slim.max_pool2d(net, [2, 2], scope=\"pool2\")\n",
    "        net = slim.conv2d(net, 256, [3, 3], scope=\"conv3\")\n",
    "        net = slim.max_pool2d(net, [2, 2], scope=\"pool3\")      \n",
    "        net = slim.flatten(net, scope=\"flatten\")\n",
    "        net = slim.fully_connected(net, 1024, scope=\"fc1\")\n",
    "        net = slim.dropout(net, 0.5, is_training=is_training, scope=\"dropout1\")\n",
    "        net = slim.fully_connected(net, 5, activation_fn=None, scope=\"fc2\")\n",
    "        \n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def create_model( lr ):\n",
    "    model = dict()\n",
    "    \n",
    "    g = tf.Graph()\n",
    "    with g.as_default():\n",
    "        inputs = tf.placeholder(tf.float32, [None, 224, 224, 3], name=\"inputs\")\n",
    "        labels = tf.placeholder(tf.int32, [None], name=\"labels\")\n",
    "        is_training = tf.placeholder(tf.bool, name=\"is_training\")\n",
    "\n",
    "        pred = build_model(inputs, is_training)\n",
    "\n",
    "        one_hot_labels = slim.one_hot_encoding(labels, 5)\n",
    "        slim.losses.softmax_cross_entropy(pred, one_hot_labels)\n",
    "        total_loss = slim.losses.get_total_loss()\n",
    "        opt = tf.train.AdamOptimizer(lr).minimize(total_loss)\n",
    "\n",
    "        config = tf.ConfigProto(allow_soft_placement=True,\n",
    "                        gpu_options=tf.GPUOptions(allow_growth=True))\n",
    "        sess = tf.Session(config=config)\n",
    "\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    model[\"inputs\"] = inputs\n",
    "    model[\"labels\"] = labels\n",
    "    model[\"is_training\"] = is_training\n",
    "    model[\"pred\"] = pred\n",
    "    model[\"loss\"] = total_loss\n",
    "    model[\"opt\"] = opt\n",
    "    model[\"sess\"] = sess\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "NUM_STEP = 50\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "def fit( model, data, verbose=True ):\n",
    "    sess, opt, total_loss = model[\"sess\"],  model[\"opt\"], model[\"loss\"]\n",
    "    inputs, labels, is_training = model[\"inputs\"], model[\"labels\"], model[\"is_training\"]\n",
    "    \n",
    "    def next_batch( indices ):\n",
    "        batch_img = np.zeros((len(indices), 224, 224, 3))\n",
    "        batch_label = np.zeros((len(indices)))\n",
    "        \n",
    "        for i, idx in enumerate(indices):\n",
    "            batch_img[i] = data[idx][\"img\"]\n",
    "            batch_label[i] = LABELS.index(data[idx][\"label\"])\n",
    "            \n",
    "        return batch_img, batch_label\n",
    "        \n",
    "        \n",
    "    for step in range(NUM_STEP):\n",
    "        indices = np.random.randint(len(data), size=BATCH_SIZE)\n",
    "        batch_img, batch_label = next_batch(indices)\n",
    "        \n",
    "        _, loss = sess.run([opt, total_loss], feed_dict={\n",
    "                inputs: batch_img, labels: batch_label, is_training:True})\n",
    "        \n",
    "        if verbose and (step+1) % 100 == 0:\n",
    "            print(step+1, loss)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def predict( data ):\n",
    "    sess, pred = model[\"sess\"], model[\"pred\"]\n",
    "    inputs, is_training = model[\"inputs\"], model[\"is_training\"]\n",
    "\n",
    "    def next_batch( indices ):\n",
    "        batch_img = np.zeros((len(indices), 224, 224, 3))\n",
    "        \n",
    "        for i, idx in enumerate(indices):\n",
    "            batch_img[i] = data[idx][\"img\"]\n",
    "            \n",
    "        return batch_img\n",
    "    \n",
    "    predicts = np.zeros((len(data), 5))\n",
    "    num_step = np.ceil(len(data)/BATCH_SIZE).astype(int)\n",
    "    for step in range(num_step):\n",
    "        start = step*BATCH_SIZE\n",
    "        end   = min(len(data), (step+1)*BATCH_SIZE)\n",
    "        indices = np.arange(start, end)\n",
    "        \n",
    "        batch_img = next_batch(indices)\n",
    "        \n",
    "        preds = sess.run(pred, feed_dict={\n",
    "                inputs: batch_img, is_training:False})\n",
    "\n",
    "        predicts[start:end] = np.reshape(preds, (-1, 5))\n",
    "        \n",
    "    return predicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def eval( pred, label ):\n",
    "    pred_argmax = np.argmax(pred, axis=1)\n",
    "    return (label==pred_argmax).sum() / len(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10,)\n",
      "0.0001 0.473206176203\n",
      "0.0011 0.242506811989\n",
      "0.0021 0.242506811989\n",
      "0.0031 0.244323342416\n",
      "0.0041 0.243415077203\n",
      "0.0051 0.246139872843\n",
      "0.0061 0.245231607629\n",
      "0.0071 0.244323342416\n",
      "0.0081 0.211625794732\n",
      "0.0091 0.245231607629\n"
     ]
    }
   ],
   "source": [
    "label_test = [ LABELS.index(data[\"test\"][idx][\"label\"]) for idx in range(len(data[\"test\"])) ]\n",
    "\n",
    "lr_list = np.arange(0.0001, 0.01, 0.001)\n",
    "print(lr_list.shape)\n",
    "for i, lr in enumerate(lr_list) :\n",
    "    model = create_model(lr=lr)\n",
    "    fit(model, data[\"train\"], verbose=False)\n",
    "    pred_test = predict(data[\"test\"])\n",
    "    \n",
    "    print(lr, eval(pred_test, label_test))\n",
    "    \n",
    "    tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
